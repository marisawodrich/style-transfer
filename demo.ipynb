{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleTransfer_IANNwTF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OChRGKq7H2y"
      },
      "source": [
        "# Artistic Style Transfer \n",
        "\n",
        "This Colab contains a replication of the artistic style transfer proposed by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge in their paper 'A Neural Algorithm of Artistic Style' (2015).\n",
        "\n",
        "*Replication by Marisa Wodrich and Johanna Linkemeyer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zsq2Bwq7QRB"
      },
      "source": [
        "##**1 Introduction**\n",
        "\n",
        "Hier kommt ein bisschen  Text, was machen wir so und wie geht das und wieso ist das hart cool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXYjAvO4T42b"
      },
      "source": [
        "##**2 Data**\n",
        "\n",
        "In this section, the data for our style transfer is loaded. Also, the functions to preprocess our data for the network are defined here. The data we use are images. More precisely, for our artistic style transfer, we require content and style images. Content images can be basically any image. We chose the Neckarfront image from the paper we replicate and, additionally, some other images we found on the internet to test the effectiveness of the style transfer on other images, too:\n",
        "* Image of the Neckarfront in Tübingen (Original paper by Gatys and colleagues)\n",
        "* A Field of Sunflowers\n",
        "* A Puppy on a Field\n",
        "* A Lake in Front of a Mountain\n",
        "\n",
        "For the style images, we chose the five style images from the original paper and added two more that we liked particularly. The style images provided from us are:\n",
        "* The Starry Night by Vincent van Gogh (1889) *- Post-Impressionism*\n",
        "* The Scream by Edvard Munch (1893) *- Expressionism*\n",
        "* Cafeterrasse am Abend by Vincent van Gogh (1888) *- Post-Impressionism*\n",
        "* Style from the Artist James Rizzy *-* \n",
        "* The Shipwreck of the Minotaur by William Turner (1810) *- Romantic*\n",
        "* Composition VII by Wassily Kadinsky (1913) *- Abstract Art*\n",
        "* Femme Nue Assise by Pablo Picasso (1910) *- Cubism*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcoTO8KwJSmn"
      },
      "source": [
        "###**2.1 Setup**\n",
        "\n",
        "This section is for setup purposes. All necessary packages are imported and the required directories to store the data are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbfKsvNrJQ9t"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import functools\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import skimage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myiqo7ZnODHU"
      },
      "source": [
        "def make_dirs(paths):\n",
        "  \"\"\"\n",
        "  Creates directories from given paths if they do not exist.\n",
        "\n",
        "  paths: (list) list of paths to directories\n",
        "  return: None\n",
        "  \"\"\"\n",
        "  \n",
        "  for path in paths:\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQlPi76YPZUV"
      },
      "source": [
        "# Define paths directories for images (subdivided into style, content and \n",
        "# generated images) if they do not exist.\n",
        "path = 'images'\n",
        "path_style = os.path.join(path, 'style')\n",
        "path_content = os.path.join(path, 'content')\n",
        "path_generated = os.path.join(path, 'generated')\n",
        "\n",
        "# Create directories from predefined paths.\n",
        "make_dirs([path, path_style, path_content, path_generated])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WggdTiGPisi"
      },
      "source": [
        "###**2.2 Load and Visualize Images**\n",
        "\n",
        "The following sections contains all functions to load images and visualize them.\n",
        "\n",
        "There is the option to add an image from your own link or your computer if you have a preferred image for style transfer. You can find the instructions to use your own image in the section **2.1.2 Load Images**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiLQv3mH-fh-"
      },
      "source": [
        "####**2.2.1 Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaW8bJK0BNIt"
      },
      "source": [
        "# This dictionary stores the image names and their respective description.\n",
        "TITLE_DICT = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybHJaFVPBTTH"
      },
      "source": [
        "def update_dictionary(image_name, image_description):\n",
        "  \"\"\"\n",
        "  Updates the dictionary that stores the image descriptions to use them \n",
        "  as titles for visualization.\n",
        "  \n",
        "  image_name: (string) name of image file\n",
        "  image_description: (string) description of image, e.g., its full title\n",
        "  return: None\n",
        "  \"\"\"\n",
        "\n",
        "  TITLE_DICT[image_name] = image_description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joES8qh7Ab3f"
      },
      "source": [
        "def test_jpg_or_png(string_to_test):\n",
        "  \"\"\"\n",
        "  Tests whether a given string ends with '.jpg' or '.png'.\n",
        "\n",
        "  string_to_test: (string) string of which the ending should be tested\n",
        "  return: (bool) True if string ends with '.jpg' or '.png', False else\n",
        "  \"\"\"\n",
        "\n",
        "  return string_to_test.endswith('.jpg') or string_to_test.endswith('.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVYrgD9r-dJm"
      },
      "source": [
        "def load_image_from_link(link, image_name, image_description, style_image=True):\n",
        "  \"\"\"\n",
        "  Load an image from a given string and save it with a given file name in \n",
        "  the style or content image directory. Also, the TITLE_DICT is updated.\n",
        "  \n",
        "  link: (string) link to the image to load\n",
        "  image_name: (string) name of the image file\n",
        "  image_description: (string) description of the image\n",
        "  style_image: (bool) whether to load a style or content image - True for \n",
        "      style image, False for content image\n",
        "  return: None\n",
        "  \"\"\"\n",
        "\n",
        "  # Test if given image_name is valid\n",
        "  assert test_jpg_or_png(image_name), 'Please provide the image name with the \\\n",
        "      ending \\'.jpg\\' or \\'.png\\'.'\n",
        "  assert test_jpg_or_png(link), 'Please provide a valid link. The given link \\\n",
        "      is not a link to an image.'\n",
        "\n",
        "  # If style_image is true, the image should be saved in the directory \n",
        "  # for style images, else saved in the directory for content images.\n",
        "  if style_image:\n",
        "    image_type = 'style'\n",
        "  else:\n",
        "    image_type = 'content'\n",
        "\n",
        "  path2dir = os.path.join('/content', 'images', image_type)\n",
        "\n",
        "  image_path = os.path.join(path2dir, image_name)\n",
        "\n",
        "  # Load image from link and convert to RGB color space\n",
        "  image = io.imread(link)\n",
        "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Save loaded image in given path\n",
        "  cv2.imwrite(image_path, image_rgb)\n",
        "\n",
        "  # Update the dictionary with image description (for plotting all \n",
        "  # loaded images)\n",
        "  update_dictionary(image_name, image_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KIZH9G4-lJ-"
      },
      "source": [
        "####**2.2.2 Load Images**\n",
        "\n",
        "Here, the functions above are used to load some images that can/ will later be used for style transfer. We provide seven style images and four content images. The images from the paper by Gatys and colleagues are included to allow a comparison of our results to theirs.\n",
        "\n",
        "If you would like to use a style or content image of your choice from a link, feel free to use the \n",
        "\n",
        ">**load_image_from_link(link, image_name, image_description, style_image)** \n",
        "\n",
        "function. For the parameters, please adapt them to your needs in the following way:\n",
        "\n",
        "* **link**: Here, you should insert the link to your image. Please use the link that explicitly leads to the image, not the website you obtain it from. To check, this link should end with **.jpg** or **.png**. For some images, even though you provide the correct link, this method of loading the image won't work. You can still use the image by saving it on your device and uploading it manually to the respective folder.\n",
        "\n",
        "* **image_name**: Here, you should insert the name you want to give the image. Please check, that the name ends with **.jpg** or **.png**.\n",
        "\n",
        "* **image_description**: Please provide a short description of the image. This description will be used as a title in the image visualization.\n",
        "\n",
        "* **style_image**: Depending on whether you would like to add a new style or content image, set this boolean to *true* or *false*. Choose *true* if you add a style image, and *false* for a content image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ip5wKZsBtOT"
      },
      "source": [
        "# ---- STYLE IMAGES ----\n",
        "\n",
        "# The Starry Night by Vincent van Gogh (1889)\n",
        "load_image_from_link('https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg',\n",
        "                     'starry_night.jpg',\n",
        "                     'Starry Night',\n",
        "                     style_image=True)\n",
        "\n",
        "# The Scream by Edvard Munch (1893)\n",
        "load_image_from_link('https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg/300px-Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg',\n",
        "                     'the_scream.jpg',\n",
        "                     'The Scream',\n",
        "                     style_image=True)\n",
        "\n",
        "# Cafeterrasse am Abend by Vincent van Gogh (1888)\n",
        "load_image_from_link('https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Gogh4.jpg/300px-Gogh4.jpg',\n",
        "                     'cafeterrasse_am_abend.jpg',\n",
        "                     'Caféterrasse am Abend',\n",
        "                     style_image=True)\n",
        "\n",
        "# Style from the Artist James Rizzy\n",
        "load_image_from_link('http://www.james-rizzi.com/wp-content/uploads/pictures/cache/2007_03_000_KeepingBusyInARizziCity_800_600.jpg',\n",
        "                     'rizzy.jpg',\n",
        "                     'Rizzy Style',\n",
        "                     style_image=True)\n",
        "\n",
        "# The Shipwreck of the Minotaur by William Turner (1810)\n",
        "load_image_from_link('https://upload.wikimedia.org/wikipedia/commons/2/2e/Shipwreck_turner.jpg',\n",
        "                     'shipwreck.jpg',\n",
        "                     'The Shipwreck of the Minotaur',\n",
        "                     style_image=True)\n",
        "\n",
        "# Composition VII by Wassily Kadinsky (1913)\n",
        "load_image_from_link('https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg',\n",
        "                     'composition_vii.jpg',\n",
        "                     'Composition VII',\n",
        "                     style_image=True)\n",
        "\n",
        "# Femme Nue Assise by Pablo Picasso (1910)\n",
        "load_image_from_link('https://az334033.vo.msecnd.net/images-7/seated-nude-femme-nue-assise-pablo-picasso-1909-f9095482.jpg',\n",
        "                     'femme_nue_assise.jpg',\n",
        "                     'Femme nue assise',\n",
        "                     style_image=True)\n",
        "\n",
        "# ---- CONTENT IMAGES ----\n",
        "\n",
        "# Image of a Field of Sunflowers\n",
        "load_image_from_link('https://www.myhomebook.de/data/uploads/2020/02/gettyimages-1141659565-1024x683.jpg',\n",
        "                     'sunflower.jpg',\n",
        "                     'Sunflower',\n",
        "                     style_image=False)\n",
        "\n",
        "# Image of a Lake in Front of a Mountain\n",
        "load_image_from_link('https://www.scinexx.de/wp-content/uploads/0/1/01-35131-nukliduhr01.jpg',\n",
        "                     'mountain.jpg',\n",
        "                     'Mountain',\n",
        "                     style_image=False)\n",
        "\n",
        "# Image of a Puppy on a Field\n",
        "load_image_from_link('https://www.mera-petfood.com/files/_processed_/a/4/csm_iStock-521697453_7570f7a9b6.jpg',\n",
        "                     'puppy.jpg',\n",
        "                     'Puppy',\n",
        "                     style_image=False)\n",
        "\n",
        "# Image of the Neckarfront in Tübingen\n",
        "load_image_from_link('https://upload.wikimedia.org/wikipedia/commons/0/00/Tuebingen_Neckarfront.jpg',\n",
        "                     'neckarfront.jpg',\n",
        "                     'Neckarfront',\n",
        "                     style_image=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OvSOQ1-CCs1"
      },
      "source": [
        "####**2.2.3 Image Visualization**\n",
        "\n",
        "Here, one can take a look at the loaded data. Running the following two code blocks will visualize all style and content images that are in the respective folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLeAM7e1kFb7"
      },
      "source": [
        "def visualize_images(path):\n",
        "  \"\"\"\n",
        "  This function creates a plot of all images contained in the directory (given \n",
        "  path).\n",
        "  \n",
        "  path: (string) path to directory of all images to be plotted\n",
        "  return: None\n",
        "  \"\"\"\n",
        "  \n",
        "  assert os.path.exists(path), 'Given path does not exist.'\n",
        "  assert os.listdir(path), 'Given path is empty.'\n",
        "\n",
        "  path_content = os.listdir(path)\n",
        "\n",
        "  # Remove files that are not images (This can be seen as a step of caution \n",
        "  # but usually path_content and imgs should not differ because we only load \n",
        "  # images to the folder).\n",
        "  imgs = [element for element in path_content if element.endswith('.jpg') or \\\n",
        "          element.endswith('.png')]\n",
        "\n",
        "  # Test if images were found, so plotting does not run into errors.\n",
        "  assert imgs, 'No images found in given path. Please repeat the loading process.'\n",
        "\n",
        "  if len(imgs) > 1:\n",
        "    _, ax = plt.subplots(1, len(imgs), figsize=(len(imgs)*3,len(imgs)*2))\n",
        "  elif len(imgs) == 1:\n",
        "    _, ax = plt.subplots(1, figsize=(len(imgs)*3,len(imgs)*2))\n",
        "\n",
        "  # Iterate over list of image references, load images \n",
        "  # and plot them beside each other.\n",
        "  for i, img in enumerate(imgs):\n",
        "\n",
        "    # If image name is listed in title dictionary, search for corresponding \n",
        "    # image title. Else, just use the filename without the ending as title.\n",
        "    if img in TITLE_DICT:\n",
        "      img_name = TITLE_DICT[img]\n",
        "    else:\n",
        "      img_name = img.split('.')[0]\n",
        "\n",
        "    # Load image\n",
        "    img = Image.open(os.path.join(path, img))\n",
        "\n",
        "    # Plot images (Special case for plotting if there is only one image in the \n",
        "    # given path.)\n",
        "    if len(imgs) > 1:\n",
        "      ax[i].imshow(img)\n",
        "      ax[i].axis('off')\n",
        "      ax[i].title.set_text(img_name)\n",
        "    elif len(imgs) == 1:\n",
        "      ax.imshow(img)\n",
        "      ax.axis('off')\n",
        "      ax.title.set_text(img_name)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSdzpfg7UdY1"
      },
      "source": [
        "print('STYLE IMAGES')\n",
        "visualize_images(path_style)\n",
        "\n",
        "print('\\nCONTENT IMAGES')\n",
        "visualize_images(path_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKlS_-bmdGKL"
      },
      "source": [
        "###**2.3 Preprocessing Functions**\n",
        "\n",
        "For this project, we will use pre-trained layers from VGG19, a Convolutional Neural Network (CNN) for object detection. Input images to VGG19 must be preprocessed in the following way:\n",
        "\n",
        "* Size of images: 224 x 224 x 3\n",
        "* Color range: [0, 255]\n",
        "* Value type: float 32\n",
        "* Colorspace: BGR\n",
        "* Normalization (zero-centering) per channel (B: 103.939, G: 116.779, R: 123.68)\n",
        "\n",
        "*This information is obtained from [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/preprocess_input) and [here](https://towardsdatascience.com/extract-features-visualize-filters-and-feature-maps-in-vgg16-and-vgg19-cnn-models-d2da6333edd0) and not provided in the paper by Gatys and colleagues.*\n",
        "\n",
        "NOTE: We do not process all data right away because we can do it easily when running the style transfer later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2b2kKmLwA7n"
      },
      "source": [
        "def img2tensor(path):\n",
        "  \"\"\"\n",
        "  Loads an image from a given path and preprocesses it for usage for the \n",
        "  VGG19 network as described above.\n",
        "\n",
        "  path: (string) path to an image\n",
        "  return: (tensor) preprocessed image as tensor\n",
        "  \"\"\"\n",
        "\n",
        "  # Load image as BGR, float32 image in the range [0,255]\n",
        "  img = cv2.imread(path).astype(np.float32)\n",
        "\n",
        "  # Save information about the original shape\n",
        "  orig_shape = img.shape \n",
        "\n",
        "  # Normalize (zero-center) the values according to imagenet mean of \n",
        "  # respective color channel from imagenet dataset\n",
        "  img[:, :, 0] -= 103.939\n",
        "  img[:, :, 1] -= 116.779\n",
        "  img[:, :, 2] -= 123.68\n",
        "\n",
        "  # Convert image to tensor\n",
        "  img_tensor = tf.convert_to_tensor(img)\n",
        "\n",
        "  # Resize to 224 x 224 x 3 and add a new axis for batch size\n",
        "  img_resized = tf.image.resize(img_tensor, (224,224))\n",
        "  img_resized = img_resized[tf.newaxis, :]\n",
        "\n",
        "  return img_resized, orig_shape\n",
        "\n",
        "\n",
        "def tensor2img(tensor, orig_shape):\n",
        "  \"\"\"\n",
        "  Inverse method for preprocessing function (img2tensor). This method \n",
        "  takes an image created by the network and adapts the values such that it \n",
        "  is an array in RGB colorspace, value range [0,255]. Also, the image is \n",
        "  resized to its original shape.\n",
        "\n",
        "  tensor: (tensor) input image as tensor\n",
        "  orig_shape: (array) shape of original content image\n",
        "  return: (array) image for plotting\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert tensor to array\n",
        "  img = np.array(tensor)\n",
        "\n",
        "  # Inverse operation to zero-centering in preprocessing function \n",
        "  # (image in BGR color space)\n",
        "  img[:, :, :, 0] += 103.939\n",
        "  img[:, :, :, 1] += 116.779\n",
        "  img[:, :, :, 2] += 123.68\n",
        "\n",
        "  # All values must be in range [0,255], clip in case of rounding errors\n",
        "  img_clipped = np.clip(img, 0., 255.)\n",
        "\n",
        "  # Convert image to 3-channel RGB image\n",
        "  img_rgb = img_clipped[0, :, :, ::-1].astype(np.uint8)\n",
        "\n",
        "  # Resize such that size matches that of the original image\n",
        "  img_resized = cv2.resize(img_rgb, (orig_shape[1], orig_shape[0]))\n",
        "\n",
        "  return img_resized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjZN2j5ndfF3"
      },
      "source": [
        "Testing the preprocessing pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d1Bs-rw1tW4"
      },
      "source": [
        "# Define path to test image\n",
        "img_path = os.path.join(path_content, 'puppy.jpg')\n",
        "\n",
        "# Preprocessing of given image\n",
        "img_tensor, orig_shape = img2tensor(img_path)\n",
        "\n",
        "# Test if image actually is a tensor\n",
        "assert tf.is_tensor(img_tensor), 'Image is not a tensor.'\n",
        "\n",
        "# Test if conversion back works as well\n",
        "img = tensor2img(img_tensor, orig_shape)\n",
        "\n",
        "# Show image to check if transfer back worked.\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgsHOMOG7n5X"
      },
      "source": [
        "##**3 Model**\n",
        "\n",
        "In the orginal the VGG-network is used. VGG is a Convolutional Neural Network (CNN) for object detection. We will first inspect the original model and then define our own model based on the pretrained layers (imagenet) of the VGG19 network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O09KjTTjiK7u"
      },
      "source": [
        "###**3.1 Inspect VGG19**\n",
        "\n",
        "A pretrained version of VGG19 is available from the keras applications. We load the model with the imagenet weights. We also specify that we would like to use average pooling (indicated by the parameter *pooling='avg'*) because this variant is used in the original paper. Another variant would be *pooling='max'* for max-pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P0ZPV_UE9xW"
      },
      "source": [
        "# Load the model\n",
        "model_vgg19 = tf.keras.applications.VGG19(include_top=False,\n",
        "                                          weights='imagenet',\n",
        "                                          pooling='avg')\n",
        "\n",
        "# Inspect the layers.\n",
        "for layer in model_vgg19.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dcMIPXmP7_P"
      },
      "source": [
        "We can observe that the model consists of an input layer, followed by five blocks and finished off by a global average pooling layer. Each block consists of four convolutional layers, finished with a pooling layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlRrvj5PiQNb"
      },
      "source": [
        "###**3.2 Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtaaGe0damWm"
      },
      "source": [
        "class StyleModel(Model):\n",
        "\n",
        "  def __init__(self, number_content_layers, layer_names):\n",
        "    \"\"\"\n",
        "    Initializes a Style Transfer Model from layers of the VGG19 network.\n",
        "    number_content_layers: (int) Number of used content layers\n",
        "    layer_names: (list) list of strings with layer names (that must be \n",
        "        layer names from VGG19)\n",
        "    \"\"\"\n",
        "    super(StyleModel, self).__init__()\n",
        "\n",
        "    # Using the pretrained VGG19 model (on imagenet) with average pooling \n",
        "    # layers as in the original paper.\n",
        "    vgg19 = tf.keras.applications.VGG19(include_top=False,\n",
        "                                        weights='imagenet',\n",
        "                                        pooling='avg')\n",
        "    \n",
        "    # Store to access the correct layers later. All other layers will be \n",
        "    # style layers\n",
        "    self.number_content_layers = number_content_layers\n",
        "    \n",
        "    # Content and style layers respectively\n",
        "    self.content_layers = [vgg19.get_layer(layer).output \n",
        "                           for layer in layer_names[:number_content_layers]]\n",
        "    self.style_layers = [vgg19.get_layer(layer).output \n",
        "                         for layer in layer_names[number_content_layers:]]\n",
        "    \n",
        "    # Concatenate to get all output layers\n",
        "    self.output_layers = self.content_layers + self.style_layers \n",
        "\n",
        "    # Build model\n",
        "    self.model = Model([vgg19.input], self.output_layers)\n",
        "\n",
        "  def call(self, img):\n",
        "    \"\"\"\n",
        "    Processes a given image with the Style Transfer model.\n",
        "    img: (tensor) input image, zero-centered, BGR in range [0, 255] \n",
        "    return: (list) list of outputs from each layer\n",
        "    \"\"\"\n",
        "\n",
        "    # Get list of outputs\n",
        "    outputs = self.model(img)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXkSSGxoQ90v"
      },
      "source": [
        "###**3.3 Define Gram Matrix and Loss Functions**\n",
        "\n",
        "The style transfer is achieved through a very specific loss. The overall loss consists of the sum of weighted content and style losses. The formula for the overall loss is as follows:\n",
        "<center>$L_{total}(\\vec{p}, \\vec{a}, \\vec{x}) = \\alpha * L_{content}(\\vec{p}, \\vec{x}) + \\beta * L_{style}(\\vec{a}, \\vec{x})$</center>\n",
        "\n",
        "with:\n",
        "* $\\vec{p} = $ content image/ photograph\n",
        "* $\\vec{a} = $ artwork image\n",
        "* $\\vec{x} = $ generated image\n",
        "* $\\alpha = $ weight for the content loss\n",
        "* $\\beta = $ weight for the style loss\n",
        "\n",
        "\n",
        "$\\alpha$ and $\\beta$ will be provided by the user when running the style transfer. In the following sections, there will be a detailed description on how exactly the content and style losses are calculated, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg9gMqNzvWAE"
      },
      "source": [
        "####**3.3.1 Gram Matrix**\n",
        "\n",
        "The Gram Matrix is necessary for the calculation of the style loss. The idea of the gram matrix is to get the correlation of the filter responses for each layer. Those feature correlations are provided through the inner product of the vectorized feature maps (for each layer). So, the gram matrix has a shape of $(number\\_channels * number\\_channels)$.\n",
        "\n",
        "The formula for the gram matrix is as follows:\n",
        "<center>$G_{ij}^l = \\sum_k F_{ik}^l * F_{jk}^l$</center>\n",
        "\n",
        "with:\n",
        "* $l = $ current layer\n",
        "* $G^l = $ gram matrix in layer $l$\n",
        "* $F^l = $ feature map in layer $l$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzgUyQBjrX_B"
      },
      "source": [
        "def gram_matrix(feature_maps, num_channels):\n",
        "  \"\"\"\n",
        "  Defines the gram matrix for a given layer.\n",
        "\n",
        "  feature_maps: feature maps from one layer\n",
        "  num_channels: (int) number of feature maps in given layer\n",
        "  return: gram-matrix of shape (num_channels x num_channels)\n",
        "  \"\"\"\n",
        "\n",
        "  # Vectorize feature maps\n",
        "  feature_maps_vectorized = tf.reshape(feature_maps, [-1, num_channels])\n",
        "\n",
        "  # Multiply vectorized 'feature maps' to themselves to get the gram matrix \n",
        "  # of shape (num_channels x num_channels).\n",
        "  gram_matrix = tf.matmul(feature_maps_vectorized, feature_maps_vectorized, \\\n",
        "                          transpose_a=True)\n",
        "\n",
        "  return gram_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyUh-UXuvY2R"
      },
      "source": [
        "####**3.3.2 Style loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV-9B7un2pSI"
      },
      "source": [
        "The style loss is the mean squared error of the gram matrices of output and target for each layer Outputs and targets are the outputs from the model of the generated image (output) and the original style image (target) for all relevant style layers.\n",
        "\n",
        "The loss is then computed by iterating over the outputs and targets for each style layer and calculating the error for those layers respectively. Each 'layer-error' is then weighted by the number of style layers.\n",
        "\n",
        "To get the difference for original and style images' outputs in the specific layer, the gram matrices are calculated for both. The sum of squared difference is then weighted to get the mean. The full formula is as follows.\n",
        "\n",
        "<center>$L_{style}(\\vec{a}, \\vec{x}) = \\sum_{l=0}^L w_l * E_l$</center>\n",
        "\n",
        "with\n",
        "* $\\vec{a} = $ original image/ artwork image\n",
        "* $\\vec{x} = $ generated image\n",
        "* $w = $ weighting factor 1 / number of style layers\n",
        "* and\n",
        "\n",
        "<center>$E_l = \\dfrac{1}{4 * N_l^2 * M_l^2} * \\sum_{i, j} (G_{ij}^l - A_{ij}^l)^2$</center>\n",
        "\n",
        "with\n",
        "* $L =$ all layers\n",
        "* $N_l =$ number of feature maps in layer $l$\n",
        "* $M_l =$ size of feature maps (its height * its width) in layer $l$\n",
        "* $A^l =$ gram matrix of target image in layer $l$\n",
        "* $G^l =$ gram matrix of generated image in layer $l$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GScmtlCKrYKP"
      },
      "source": [
        "def loss_style(outputs, targets):\n",
        "  \"\"\"\n",
        "  Calculates the style loss. The style loss is the weighted mean squared \n",
        "  error between the output gram matrix and target gram matrix of each \n",
        "  respective layer.\n",
        "\n",
        "  outputs: layer activations for generated image\n",
        "  targets: layer activations for target style image\n",
        "  return: (float) style loss\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize mean-squared-error (mse) loss.\n",
        "  mse = 0.\n",
        "\n",
        "  # Check if function was called with correct parameters. \n",
        "  assert len(outputs) == len(targets), 'Outputs and targets must have the \\\n",
        "      same length, which refers to the same amount of layers.'\n",
        "  \n",
        "  # Iterate over all output and target feature maps.\n",
        "  for output, target in zip(outputs, targets):\n",
        "\n",
        "    assert np.array(output).shape == np.array(target).shape, 'Shape mismatch \\\n",
        "        of output and target feature maps.'\n",
        "\n",
        "    # Get shape info: h -> height, w -> width, c -> number of channels\n",
        "    _, h, w, c = np.array(output.shape).astype(np.uint32)\n",
        "\n",
        "    # Calculate gram matrices\n",
        "    gram_outputs = gram_matrix(output, c)\n",
        "    gram_targets = gram_matrix(target, c)\n",
        "    \n",
        "    # Calculate error. Error is weighted first to get the mean error and then\n",
        "    # by the number of style layers, which is represented by 'len(outputs)'.\n",
        "    weighting_factor = 1 / (4. * c**2 * (h*w)**2)\n",
        "    error = weighting_factor * \\\n",
        "        tf.reduce_sum(tf.square(gram_outputs - gram_targets))\n",
        "    mse += (1. / len(outputs)) * error\n",
        "\n",
        "  return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MhhOj-HvMfU"
      },
      "source": [
        "####**3.3.3 Content Loss**\n",
        "\n",
        "The content loss is the mean squared error over the feature representations of the output and target per layer.\n",
        "\n",
        "<center>$L_{content}(\\vec{p}, \\vec{x}, l) = \\frac{1}{2} * \\sum_{i,j} (F_{ij}^l - P_{ij}^l)^2$</center>\n",
        "\n",
        "with\n",
        "* $\\vec{p} = $ content image/ photograph\n",
        "* $\\vec{x} = $ generated image\n",
        "* $l = $ current layer $l$\n",
        "* $P^l = $ feature representation in layer $l$ of content image\n",
        "* $F^l = $ feature representation in layer $l$ of generated image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUjW0r31rYSr"
      },
      "source": [
        "def loss_content(outputs, targets):\n",
        "  \"\"\"\n",
        "  Calculates the mean squared error (MSE) for the content activations.\n",
        "\n",
        "  outputs: layer activations of generated image\n",
        "  targets: layer activations of target content image\n",
        "  return: Mean squared error\n",
        "  \"\"\"\n",
        "\n",
        "  assert len(outputs) == len(targets), 'Length of content outputs and \\\n",
        "    targets must match!'\n",
        "\n",
        "  # Weight by number of layers\n",
        "  weight = 1 / len(outputs)\n",
        "\n",
        "  # MSE is the mean over squared deviation of output and target.\n",
        "  mse = 0\n",
        "  for output, target in zip(outputs, targets):\n",
        "\n",
        "    # Add up weighted errors\n",
        "    mse += weight * tf.reduce_mean(tf.square(output - target))\n",
        "\n",
        "  return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqlj01bL73vp"
      },
      "source": [
        "##**4 Running the Style Transfer**\n",
        "\n",
        "In style transfer, there is no real 'training' phase but rather a phase where the content image iteratively gets adapted such that it takes the style of the style image. First"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpVwXjfSdtW4"
      },
      "source": [
        "###**4.1 Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS0RpbSgFcXb"
      },
      "source": [
        "def prep_images(content_image_name, style_image_name):\n",
        "  \"\"\"\n",
        "  Function to load and preprocess exactly one content and style image.\n",
        "\n",
        "  content_image_name: (string) file name of the content image\n",
        "  style_image_name: (string) file name of the style image\n",
        "  return:\n",
        "    image_content: (tensor) preprocessed content image\n",
        "    image_style: (tensor) preprocessed style image\n",
        "    image_content_shape: (array) original shape of the content image\n",
        "  \"\"\"\n",
        "\n",
        "  # Load content image and store its original shape\n",
        "  image_content, image_content_shape = \\\n",
        "      img2tensor(os.path.join(path_content, content_image_name))\n",
        "\n",
        "  # Load style image\n",
        "  image_style, _ = img2tensor(os.path.join(path_style, style_image_name))\n",
        "\n",
        "  return image_content, image_style, image_content_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6etTXT1sQ7Qf"
      },
      "source": [
        "def clip_values(img):\n",
        "  \"\"\"\n",
        "  Clips the values of a tensor image to the range of the respective color \n",
        "  channel. This is necessary because of the zero-centering of each image in \n",
        "  the preprocessing step.\n",
        "\n",
        "  img: (tensor) tensor-image to clip the values\n",
        "  return: (tensor) clipped version of tensor-image\n",
        "  \"\"\"\n",
        "\n",
        "  norm_means = np.array([103.939, 116.779, 123.68])\n",
        "  min_vals = -norm_means\n",
        "  max_vals = 255 - norm_means   \n",
        "\n",
        "  return tf.clip_by_value(img, clip_value_min=min_vals, clip_value_max=max_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbDZ_H16H_5W"
      },
      "source": [
        "def visualize_progress(image_names):\n",
        "  \"\"\"\n",
        "  Visualizes progress of style transfer for the latest combination. \n",
        "  (Images in generated image folder will be overwritten in each run to prevent \n",
        "  memory issues.)\n",
        "\n",
        "  image_names: (list) list of strings with all image names of intermediate \n",
        "    images\n",
        "  return: None\n",
        "  \"\"\"\n",
        "\n",
        "  # Path to generated images\n",
        "  gen_path = os.path.join('/content', 'images', 'generated')\n",
        "\n",
        "  # Iterate over all saved image names.\n",
        "  for img_name in image_names:\n",
        "\n",
        "    # Load image from directory of generated images\n",
        "    img = plt.imread(os.path.join(gen_path, img_name))\n",
        "\n",
        "    # Wait after each plot and clear old plot to get progress plot\n",
        "    time.sleep(1)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Show image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Iteration: ' + img_name.split('.')[0])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy_aVclmdz39"
      },
      "source": [
        "###**4.2 Define Model with Specific Layers and Default Parameters**\n",
        "\n",
        "For the content layers of our model for artistic style transfer, we chose 'block4_conv2' as in the original paper.\n",
        "\n",
        "For the style layers, we chose the variant from the original paper. From each block, the first convolutional layer is used in the style transfer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGBfWyzukdqT"
      },
      "source": [
        "# Content layers: Second convolutional layer from the forth block.\n",
        "layers_content = ['block4_conv2'] \n",
        "\n",
        "# Style layers: First convolutional layer of each of the five blocks\n",
        "layers_style = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "\n",
        "assert len(layers_content) != 0, 'There must be at least one content layer.'\n",
        "assert len(layers_style) != 0, 'There must be at least one style layer.'\n",
        "\n",
        "# Append the layer names to define the full model.\n",
        "layers_complete = layers_content + layers_style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdCdiiM9Kqyg"
      },
      "source": [
        "# Set model with the previously defined layers\n",
        "model = StyleModel(number_content_layers=len(layers_content), \n",
        "                   layer_names=layers_complete)\n",
        "\n",
        "# Model should not be trainable\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# --- Default Parameters ---\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate=4)\n",
        "\n",
        "# Weights\n",
        "weight_content_default = 1\n",
        "weight_style_default = 1e-2\n",
        "\n",
        "# Image names\n",
        "image_content_name = 'neckarfront.jpg'\n",
        "image_style_name = 'composition_vii.jpg'\n",
        "save_name_default = image_content_name.split('.')[0] + '-' + image_style_name\n",
        "\n",
        "# Load images\n",
        "image_content_default, image_style_default, image_content_default_shape = \\\n",
        "    prep_images(image_content_name, image_style_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awS7iZJ4d3vS"
      },
      "source": [
        "###**4.3 Style Transfer Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kfiPo9chsGM"
      },
      "source": [
        "def transfer_artistic_style(iterations=300, \n",
        "                            content=image_content_default, \n",
        "                            style=image_style_default, \n",
        "                            optimizer=optimizer, \n",
        "                            weight_content=weight_content_default, \n",
        "                            weight_style=weight_style_default,\n",
        "                            content_orig_shape=(224,224,3),\n",
        "                            save_name='final.jpg',\n",
        "                            visualize_intermediate=False):\n",
        "  \"\"\"\n",
        "  This function runs the style transfer on a given content image with a given \n",
        "  style image.\n",
        "\n",
        "  iterations: (int) number of iterations of style transform\n",
        "  content: (tensor) preprocessed content image\n",
        "  style: (tensor) preprocessed style image\n",
        "  optimizer: optimizer for model\n",
        "  weight_content: (float) weight for the content loss\n",
        "  weight_style: (float) weight for the style loss\n",
        "  content_orig_shape: (array) shape of the original content image\n",
        "  save_name: (string) name of the result image\n",
        "  visualize_intermediate: (bool) whether to visualize during the transfer \n",
        "    progress\n",
        "  return:\n",
        "    (list) list of strings containing the names of the intermediate images\n",
        "  \"\"\"\n",
        "  \n",
        "  print('Start')\n",
        "\n",
        "  # Create an image to apply the operations to (like content image)\n",
        "  image_generated = tf.Variable(content, dtype=np.float32)\n",
        "  img_gen = tensor2img(image_generated, content_orig_shape)\n",
        "\n",
        "  # Get the content layers (first layer returned by model)\n",
        "  target_content = model(content)[:model.number_content_layers]\n",
        "\n",
        "  # Get the style layers (all layers except the first for our case)\n",
        "  target_style = model(style)[model.number_content_layers:]\n",
        "\n",
        "  # Initialize list to store image names for plotting the progress later.\n",
        "  intermediate_img_names = []\n",
        "\n",
        "  print('Got target images, start optimizing.')\n",
        "\n",
        "  start_time = time.time()\n",
        "  time_intermediate = 0\n",
        "\n",
        "  # Iteratively adapt the generated image to match the style image.\n",
        "  for i in range(iterations):\n",
        "\n",
        "    # Compute gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Process image with model\n",
        "      output = model(image_generated)\n",
        "\n",
        "      # Again, output of the first layer is content output, all other layers \n",
        "      # are style outputs in our case\n",
        "      output_content = output[:model.number_content_layers]\n",
        "      output_style = output[model.number_content_layers:]\n",
        "\n",
        "      # Calculate loss with given weights\n",
        "      content_loss = loss_content(output_content, target_content)\n",
        "      style_loss = loss_style(output_style, target_style)\n",
        "      loss = (weight_content * content_loss) + (weight_style * style_loss)\n",
        "\n",
        "    # Gradients with respect to generated image\n",
        "    gradients = tape.gradient(loss, image_generated)\n",
        "\n",
        "    # Generate optimized image (apply gradients to image)\n",
        "    optimizer.apply_gradients([(gradients, image_generated)])\n",
        "\n",
        "    # Assign new values image, but clip values before if necessary\n",
        "    image_generated.assign(clip_values(image_generated))\n",
        "\n",
        "    time_intermediate += time.time() - start_time - time_intermediate\n",
        "\n",
        "    # Keep track of the progress. The progress information will be given \n",
        "    # 20 times: some information and (optionally) an intermediate image \n",
        "    # will be printed/ plotted.\n",
        "    if i != 0 and int(i % (iterations/20)) == 0:\n",
        "      \n",
        "      # Print information\n",
        "      print('Iteration: ', str(i), '--- Time passed: ', \n",
        "            round(time_intermediate, 2), 'seconds --- Loss: ', \n",
        "            np.array(loss).astype(np.float32), '--- Content Loss: ', \n",
        "            np.array(content_loss).astype(np.float32), '--- Style Loss: ', \n",
        "            np.array(style_loss).astype(np.float32))\n",
        "      \n",
        "      # Convert tensor image back to 'normal' image\n",
        "      img_gen = tensor2img(image_generated, content_orig_shape)\n",
        "\n",
        "      # Convert to BGR, save image with cv2, and keep track of names of \n",
        "      # intermediate images.\n",
        "      intermediate_img = cv2.cvtColor(img_gen, cv2.COLOR_RGB2BGR)\n",
        "      intermediate_img_names.append(str(i) + '.jpg')\n",
        "      cv2.imwrite(os.path.join(path_generated, str(i) + '.jpg'), \\\n",
        "                  intermediate_img)\n",
        "\n",
        "      if visualize_intermediate:\n",
        "        # Show intermediate result\n",
        "        plt.imshow(img_gen)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "  \n",
        "  # Save result image locally\n",
        "  final_img = cv2.cvtColor(img_gen, cv2.COLOR_RGB2BGR)\n",
        "  cv2.imwrite(os.path.join(path_generated, save_name), final_img)\n",
        "\n",
        "  # Print time for complete otimization process\n",
        "  print('Finished', str(iterations), 'iterations in', \n",
        "        round(time.time() - start_time, 2), 'seconds.')\n",
        "\n",
        "  return np.array(intermediate_img_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppuuTHGFH-z-"
      },
      "source": [
        "img_names = transfer_artistic_style(2, \n",
        "                                    image_content_default, \n",
        "                                    image_style_default, \n",
        "                                    optimizer, \n",
        "                                    weight_content_default,\n",
        "                                    weight_style_default,\n",
        "                                    image_content_default_shape, \n",
        "                                    save_name_default,\n",
        "                                    visualize_intermediate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvAwuQhDfqsA"
      },
      "source": [
        "# Run visualization code\n",
        "visualize_progress(img_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy9QpYTFgG1x"
      },
      "source": [
        "##**5 Results**\n",
        "\n",
        "To get all results, we first run the style transfer on all possible combinations on the content images we chose and then on all combinations from the original paper. The names of the result images are stored and can later be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvkSL0WGhtwx"
      },
      "source": [
        "def run_all_combinations(content_images, \n",
        "                         style_images, \n",
        "                         optimizer, \n",
        "                         weight_content, \n",
        "                         weight_style, \n",
        "                         num_iterations=1000):\n",
        "  \"\"\"\n",
        "  Runs style transfer on all possible combinations of given content and \n",
        "  style images.\n",
        "\n",
        "  content_images: (list) list of strings of content image file names\n",
        "  style_images: (list) list of strings of style image file names\n",
        "  return: (list) list of strings with result image names\n",
        "  \"\"\"\n",
        "  save_names = []\n",
        "\n",
        "  # Iterate over content image\n",
        "  for content_name in content_images:\n",
        "\n",
        "    # For each content image, get the combination with each style image\n",
        "    for style_name in style_images:\n",
        "\n",
        "      # Keep track of progress\n",
        "      print('Next image...')\n",
        "\n",
        "      # Load and preprocess images\n",
        "      image_content, image_style, image_content_shape = \\\n",
        "          prep_images(content_name, style_name)\n",
        "\n",
        "      # Define name to save result image\n",
        "      save_name = content_name.split('.')[0] + '-' + style_name\n",
        "      save_names.append([content_name, style_name, save_name])\n",
        "\n",
        "      # Run artistic style transfer\n",
        "      transfer_artistic_style(num_iterations, \n",
        "                              image_content, \n",
        "                              image_style, \n",
        "                              optimizer, \n",
        "                              weight_content, \n",
        "                              weight_style, \n",
        "                              image_content_shape,\n",
        "                              save_name)\n",
        "    \n",
        "  return save_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeiR7PbQjcm_"
      },
      "source": [
        "def plot_combinations(save_names):\n",
        "  \"\"\"\n",
        "  Plots all combinations and the original images.\n",
        "\n",
        "  save_names: (list) list of list that contains for each combination \n",
        "      the name of the content image, name of the style image and name of \n",
        "      combined images (transferred style)\n",
        "  return: None\n",
        "  \"\"\"\n",
        "\n",
        "  # Iterate over all result image names\n",
        "  for save_name in save_names:\n",
        "    \n",
        "    # Load images from their directory\n",
        "    img_content = plt.imread(os.path.join(path_content, save_name[0]))\n",
        "    img_style = plt.imread(os.path.join(path_style, save_name[1]))\n",
        "    img_combined = plt.imread(os.path.join(path_generated, save_name[2]))\n",
        "\n",
        "    _, ax = plt.subplots(1, 3, figsize=(8,2))\n",
        "\n",
        "    # Plot images\n",
        "    ax[0].imshow(img_content), ax[0].axis('off')\n",
        "    ax[1].imshow(img_style), ax[1].axis('off')\n",
        "    ax[2].imshow(img_combined), ax[2].axis('off')\n",
        "\n",
        "    plt.suptitle(TITLE_DICT[save_name[0]] + ' + ' + TITLE_DICT[save_name[1]])\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqqgxp2nhjNU"
      },
      "source": [
        "###**5.1 Results on Selected Content Images**\n",
        "\n",
        "Here are the results of all possible combinations of the puppy, sunflower, and mountain content images with the style images: the scream, the starry night, Caféterrasse am Abend, and James Rizzy style.\n",
        "\n",
        "The first cell will run the code for all possible combinations and the second cell will visualize the original content and style image and the generated image next to each other for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqP_nCgEHTIL"
      },
      "source": [
        "# Define the our content image names and the the style images we would like\n",
        "content_images = ['puppy.jpg', 'sunflower.jpg', 'mountain.jpg']\n",
        "style_images = ['the_scream.jpg', \n",
        "                'cafeterrasse_am_abend.jpg', \n",
        "                'starry_night.jpg', \n",
        "                'rizzy.jpg']\n",
        "\n",
        "# Run style transfer on all those possible combinations\n",
        "save_names_own_content_imgs = run_all_combinations(content_images, \n",
        "                                                   style_images,\n",
        "                                                   optimizer,\n",
        "                                                   weight_content_default,\n",
        "                                                   weight_style_default,\n",
        "                                                   2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL9oY5PWjaYt"
      },
      "source": [
        "# Visualization\n",
        "plot_combinations(save_names_own_content_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu_2ajFQjC05"
      },
      "source": [
        "###**5.2 Results on Original Combinations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzEI7TgyNer4"
      },
      "source": [
        "# Define content and style images from original paper\n",
        "content_images = ['neckarfront.jpg']\n",
        "style_images = ['starry_night.jpg', \n",
        "                'composition_vii.jpg', \n",
        "                'the_scream.jpg', \n",
        "                'femme_nue_assise.jpg', \n",
        "                'shipwreck.jpg']\n",
        "\n",
        "# Run style transfer on all combinations from the original paper\n",
        "save_names_original_paper = run_all_combinations(content_images,\n",
        "                                                 style_images,\n",
        "                                                 optimizer,\n",
        "                                                 weight_content_default,\n",
        "                                                 1e-2,\n",
        "                                                 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gp0jThFlVeC"
      },
      "source": [
        "plot_combinations(save_names_original_paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEjbtv0M82ms"
      },
      "source": [
        "##**6 Comparison to Original Paper**\n",
        "\n",
        "Please refer to our paper [here](https://github.com/marisawodrich/style-transfer/blob/main/style_transfer_not_finished.pdf) for a thorough analysis of our results and analyses and a comparison of those to the orgininal paper by Gatys and colleagues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZdFn1EbKxB3"
      },
      "source": [
        "##**References**\n",
        "\n",
        "* Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576.\n",
        "\n",
        "* Neural Style Transfer with tf.keras. https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb#scrollTo=sElaeNX-4Vnc [accessed 2021-04-03]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGZ_3yX36w4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}